{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjKi9cFSDXT0fD74boOhx2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RAGISHIVANAND/DEEP_LEARNING/blob/main/KERNEL_CNN_ADV_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.layers import Dense,Activation,Flatten,Conv2D,MaxPooling2D\n",
        "from keras.layers import BatchNormalization,Dropout\n",
        "\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.initializers import he_normal\n",
        "\n",
        "# Example of using He Normal initialization for a dense layer\n",
        "#dense_layer = Dense(units=64, activation='relu', kernel_initializer=he_normal(seed=42))\n"
      ],
      "metadata": {
        "id": "FVBEdyRjg_6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train,y_train),(x_test,y_test)=mnist.load_data()\n",
        "\n",
        "#reshaping x data :(n,28,28)=>(n,28,28,1)\n",
        "\n",
        "x_train=x_train.reshape((x_train.shape[0],x_train.shape[1],x_train.shape[2],1))\n",
        "\n",
        "x_test=x_test.reshape((x_test.shape[0],x_test.shape[1],x_test.shape[2],1))\n",
        "\n",
        "#converting y data into categorical (one-hot encoding)\n",
        "\n",
        "y_train=to_categorical(y_train)\n",
        "y_test=to_categorical(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3aDH6hMhFWQ",
        "outputId": "9e157bb8-41f8-458e-dd09-b22b16c13350"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqG6OMJTg9Y-"
      },
      "outputs": [],
      "source": [
        "def deep_cnn_advanced_nin():\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(input_shape=(x_train.shape[1], x_train.shape[2], x_train.shape[3]),\n",
        "                     filters=50, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_initializer='he_normal'))\n",
        "\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Conv2D(filters=50, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_initializer='he_normal'))\n",
        "    model.add(Conv2D(filters=25, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_initializer='he_normal'))\n",
        "\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Conv2D(filters=50, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_initializer='he_normal'))\n",
        "    model.add(Conv2D(filters=25, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_initializer='he_normal'))\n",
        "\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Conv2D(filters=50, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_initializer='he_normal'))\n",
        "    model.add(Conv2D(filters=25, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_initializer='he_normal'))\n",
        "\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Conv2D(filters=50, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_initializer='he_normal'))\n",
        "    model.add(Conv2D(filters=25, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_initializer='he_normal'))\n",
        "\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Conv2D(filters=50, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_initializer='he_normal'))\n",
        "    model.add(Conv2D(filters=25, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_initializer='he_normal'))\n",
        "\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # prior layer should be flattened to be connected to dense layer\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # dense layer with 50 neurons\n",
        "    model.add(Dense(50, activation='relu', kernel_initializer='he_normal'))\n",
        "\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    # final layer with 10 neurons to classify the instances\n",
        "    model.add(Dense(10, activation='softmax', kernel_initializer='he_normal'))\n",
        "\n",
        "    adam = Adam(learning_rate=0.001)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = deep_cnn_advanced_nin()\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train,y_train,batch_size=50,validation_split=0.2, epochs=100, verbose=0)\n"
      ],
      "metadata": {
        "id": "6boEKDWChMkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the training history\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "train_accuracy = history.history['accuracy']\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "# Plot learning curves\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_loss, label='Train Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accuracy, label='Train Accuracy')\n",
        "plt.plot(val_accuracy, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JmJmmDHThNMz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}